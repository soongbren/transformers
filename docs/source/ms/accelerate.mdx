<!--Hak Cipta 2022 Pasukan HuggingFace. Hak cipta terpelihara.

Dilesenkan di bawah Lesen Apache, Versi 2.0 ("Lesen"); Anda tidak boleh menggunakan fail ini kecuali dengan mematuhi
Lesen. Anda boleh mendapatkan salinan Lesen di

http://www.apache.org/licenses/LICENSE-2.0

Melainkan diperlukan oleh undang-undang yang terpakai atau dipersetujui secara bertulis, perisian yang diedarkan di bawah Lesen diedarkan pada
ASAS "SEBAGAIMANA ADANYA", TANPA WARANTI ATAU SEBARANG JENIS SYARAT, sama ada nyata atau tersirat. Lihat Lesen untuk
bahasa tertentu yang mengawal kebenaran dan pengehadan di bawah Lesen.
-->

# Latihan yang diedarkan dengan ðŸ¤— Accelerate

Apabila model semakin besar, paralelisme telah muncul sebagai strategi untuk melatih model yang lebih besar pada perkakasan terhad dan mempercepatkan kelajuan latihan dengan beberapa urutan magnitud. Di Hugging Face, kami mencipta perpustakaan [ðŸ¤— Accelerate](https://huggingface.co/docs/accelerate) untuk membantu pengguna melatih model Transformers dengan mudah pada sebarang jenis persediaan yang diedarkan, sama ada berbilang GPU pada satu mesin atau berbilang GPU merentas beberapa mesin. Dalam tutorial ini, ketahui cara menyesuaikan gelung latihan PyTorch asli anda untuk mendayakan latihan dalam persekitaran yang diedarkan.

## Persediaan

Mulakan dengan memasang ðŸ¤— Accelerate:

```bash
pip install accelerate
```

Kemudian import dan cipta objek [`~accelerate.Accelerator`]. [`~accelerate.Accelerator`] akan secara automatik mengesan jenis persediaan diedarkan anda dan memulakan semua komponen yang diperlukan untuk latihan. Anda tidak perlu meletakkan model anda secara eksplisit pada peranti.

```py
>>> from accelerate import Accelerator

>>> accelerator = Accelerator()
```

## Bersedia untuk accelerate

Langkah seterusnya ialah menghantar semua objek latihan yang berkaitan kepada kaedah [`~accelerate.Accelerator.prepare`]. Ini termasuk latihan dan penilaian DataLoaders, model dan pengoptimum:

```py
>>> train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
...     train_dataloader, eval_dataloader, model, optimizer
... )
```

## Backward

Penambahan terakhir adalah untuk menggantikan `loss.backward()` biasa dalam gelung latihan anda dengan kaedah ðŸ¤— Accelerate [`~accelerate.Accelerator.backward`]:

```py
>>> for epoch in range(num_epochs):
...     for batch in train_dataloader:
...         outputs = model(**batch)
...         loss = outputs.loss
...         accelerator.backward(loss)

...         optimizer.step()
...         lr_scheduler.step()
...         optimizer.zero_grad()
...         progress_bar.update(1)
```

Seperti yang anda lihat dalam kod berikut, anda hanya perlu menambah empat baris kod tambahan pada gelung latihan anda untuk mendayakan latihan teragih!

```diff
+ from accelerate import Accelerator
  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
  optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = 3
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      "linear",
      optimizer=optimizer,
      num_warmup_steps=0,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  for epoch in range(num_epochs):
      for batch in train_dataloader:
-         batch = {k: v.to(device) for k, v in batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(1)
```

## Train

Setelah anda menambahkan baris kod yang berkaitan, lancarkan latihan anda dalam skrip atau buku nota seperti Colaboratory.

### Train with a script


Jika anda menjalankan latihan anda daripada skrip, jalankan arahan berikut untuk mencipta dan menyimpan fail konfigurasi:

```bash
accelerate config
```

Kemudian lancarkan latihan anda dengan:

```bash
accelerate launch train.py
```

### Train with a notebook

ðŸ¤— Accelerate juga boleh dijalankan dalam buku nota jika anda merancang untuk menggunakan TPU Colaboratory. Balut semua kod yang bertanggungjawab untuk latihan dalam fungsi, dan hantarkannya kepada [`~accelerate.notebook_launcher`]:

```py
>>> from accelerate import notebook_launcher

>>> notebook_launcher(training_function)
```

Untuk mendapatkan maklumat lanjut tentang ðŸ¤— Accelerate dan ciri yang kaya, rujuk pada [documentation](https://huggingface.co/docs/accelerate).